{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e260f70-1dee-40cb-8a64-ec62c79f363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13c8d691-efe8-4ae5-bb11-187026c13c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "file_path = \"C:/Users/hp/OneDrive/Desktop/aapl_dataset.csv\"  # Change to actual path\n",
    "df = pd.read_csv(file_path, parse_dates=[\"Date\"], index_col=\"Date\")\n",
    "df = df.rename({'close_aapl': 'Close'}, axis=1)\n",
    "df = df[['Close']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca8da3ed-f330-4f21-9e23-b404c8020aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split (before scaling to avoid leakage)\n",
    "train_size = int(len(df) * 0.8)\n",
    "train, test = df[:train_size], df[train_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d722fbd-3b70-4fc5-a8a6-0081d428d422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply MinMaxScaler separately on train & test\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "train_scaled = scaler.fit_transform(train)\n",
    "test_scaled = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11aa065f-94f9-413d-9ea9-ba8fb58b8d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to supervised learning format\n",
    "def create_sequences(data, time_steps=10):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i:i+time_steps])  # Input sequence\n",
    "        y.append(data[i+time_steps])    # Target value\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13b183b0-96cb-4662-a1b3-f5a8022279b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 10  # Define time steps\n",
    "X_train, y_train = create_sequences(train_scaled, time_steps)\n",
    "X_test, y_test = create_sequences(test_scaled, time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b54fc008-1744-4617-92d6-9ed291fa50e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m557/557\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 21ms/step - loss: 0.0049 - val_loss: 3.1274\n",
      "Epoch 2/15\n",
      "\u001b[1m557/557\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - loss: 9.6707e-05 - val_loss: 2.4741\n",
      "Epoch 3/15\n",
      "\u001b[1m557/557\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 20ms/step - loss: 9.3541e-05 - val_loss: 2.0817\n",
      "Epoch 4/15\n",
      "\u001b[1m557/557\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 20ms/step - loss: 9.7693e-05 - val_loss: 1.3903\n",
      "Epoch 5/15\n",
      "\u001b[1m557/557\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 20ms/step - loss: 6.8689e-05 - val_loss: 1.1447\n",
      "Epoch 6/15\n",
      "\u001b[1m557/557\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 20ms/step - loss: 5.1533e-05 - val_loss: 1.0346\n",
      "Epoch 7/15\n",
      "\u001b[1m557/557\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 20ms/step - loss: 5.2016e-05 - val_loss: 0.7263\n",
      "Epoch 8/15\n",
      "\u001b[1m557/557\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - loss: 4.8196e-05 - val_loss: 0.4972\n",
      "Epoch 9/15\n",
      "\u001b[1m557/557\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 15ms/step - loss: 4.1826e-05 - val_loss: 0.5048\n",
      "Epoch 10/15\n",
      "\u001b[1m557/557\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 20ms/step - loss: 3.8412e-05 - val_loss: 0.2773\n",
      "Epoch 11/15\n",
      "\u001b[1m557/557\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 20ms/step - loss: 4.3772e-05 - val_loss: 0.2494\n",
      "Epoch 12/15\n",
      "\u001b[1m557/557\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 3.2949e-05 - val_loss: 0.2569\n",
      "Epoch 13/15\n",
      "\u001b[1m557/557\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 20ms/step - loss: 4.4290e-05 - val_loss: 0.2349\n",
      "Epoch 14/15\n",
      "\u001b[1m557/557\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - loss: 4.0171e-05 - val_loss: 0.2587\n",
      "Epoch 15/15\n",
      "\u001b[1m557/557\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 3.5249e-05 - val_loss: 0.2668\n",
      "\u001b[1m279/279\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step\n",
      "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n"
     ]
    }
   ],
   "source": [
    "# Define LSTM Model\n",
    "model = Sequential([\n",
    "    LSTM(150, return_sequences=False, input_shape=(time_steps, 1)),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dense(1)  # Output layer\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "# Train LSTM\n",
    "model.fit(X_train, y_train, epochs=15, batch_size=16, verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "# Extract LSTM features (hidden states) for training XGBoost\n",
    "train_features = model.predict(X_train)\n",
    "test_features = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4326191-a79b-4471-b15a-3b66d49b0edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost on LSTM features\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=500, learning_rate=0.01, max_depth=5, objective='reg:squarederror')\n",
    "xgb_model.fit(train_features, y_train)\n",
    "\n",
    "# Predict using XGBoost\n",
    "xgb_predictions = xgb_model.predict(test_features)\n",
    "\n",
    "# Convert predictions back to original scale\n",
    "y_test_original = scaler.inverse_transform(y_test.reshape(-1, 1))  # Rescale actual values\n",
    "xgb_predictions_original = scaler.inverse_transform(xgb_predictions.reshape(-1, 1))  # Rescale predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78bc0025-697f-49ac-a3f0-1290c0cbbf2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Hybrid LSTM + XGBoost Model Performance:\n",
      "ğŸ”¹ MSE: 10483.6808\n",
      "ğŸ”¹ RMSE: 102.3898\n",
      "ğŸ”¹ MAPE: 57.0732%\n",
      "ğŸ”¹ RÂ²: -1.3974\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(y_test_original, xgb_predictions_original)\n",
    "rmse = np.sqrt(mse)\n",
    "mape = mean_absolute_percentage_error(y_test_original, xgb_predictions_original) * 100\n",
    "r2 = r2_score(y_test_original, xgb_predictions_original)\n",
    "\n",
    "print(f\"âœ… Hybrid LSTM + XGBoost Model Performance:\")\n",
    "print(f\"ğŸ”¹ MSE: {mse:.4f}\")\n",
    "print(f\"ğŸ”¹ RMSE: {rmse:.4f}\")\n",
    "print(f\"ğŸ”¹ MAPE: {mape:.4f}%\")\n",
    "print(f\"ğŸ”¹ RÂ²: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818c5416-fb43-4e24-9727-dc288e49db28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
